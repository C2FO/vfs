package s3

import (
	"context"
	"errors"
	"fmt"
	"io"
	"net/url"
	"os"
	"path"
	"strings"
	"time"

	"github.com/aws/aws-sdk-go/aws/awserr"
	"github.com/aws/aws-sdk-go/service/s3"
	"github.com/aws/aws-sdk-go/service/s3/s3iface"
	"github.com/aws/aws-sdk-go/service/s3/s3manager"
	"github.com/aws/aws-sdk-go/service/s3/s3manager/s3manageriface"

	"github.com/c2fo/vfs/v6"
	"github.com/c2fo/vfs/v6/mocks"
	"github.com/c2fo/vfs/v6/options"
	"github.com/c2fo/vfs/v6/options/delete"
	"github.com/c2fo/vfs/v6/utils"
)

const defaultPartitionSize = int64(32 * 1024 * 1024)

// File implements vfs.File interface for S3 fs.
type File struct {
	fileSystem *FileSystem
	bucket     string
	key        string

	// seek-related fields
	cursorPos  int64
	seekCalled bool

	// read-related fields
	reader      io.ReadCloser
	readCalled  bool
	readEOFSeen bool

	// write-related fields
	tempFileWriter     *os.File
	s3Writer           *io.PipeWriter
	cancelFunc         context.CancelFunc
	writeCalled        bool
	s3WriterCompleteCh chan struct{}
}

// Info Functions

// LastModified returns the LastModified property of a HEAD request to the s3 object.
func (f *File) LastModified() (*time.Time, error) {
	head, err := f.getHeadObject()
	if err != nil {
		return nil, err
	}
	return head.LastModified, nil
}

// Name returns the name portion of the file's key property. IE: "file.txt" of "s3://some/path/to/file.txt
func (f *File) Name() string {
	return path.Base(f.key)
}

// Path return the directory portion of the file's key. IE: "path/to" of "s3://some/path/to/file.txt
func (f *File) Path() string {
	return utils.EnsureLeadingSlash(f.key)
}

// Exists returns whether (boolean) the object exists on s3, based on a call for
// the object's HEAD through the s3 API.
func (f *File) Exists() (bool, error) {
	_, err := f.getHeadObject()
	if err != nil {
		if errors.Is(err, vfs.ErrNotExist) {
			return false, nil
		}
		return false, err
	}

	return true, nil
}

// Size returns the ContentLength value from an S3 HEAD request on the file's object.
func (f *File) Size() (uint64, error) {
	head, err := f.getHeadObject()
	if err != nil {
		return 0, err
	}
	return uint64(*head.ContentLength), nil
}

// Location returns a vfs.Location at the location of the object. IE: if file is at
// s3://bucket/here/is/the/file.txt the location points to s3://bucket/here/is/the/
func (f *File) Location() vfs.Location {
	return vfs.Location(&Location{
		fileSystem: f.fileSystem,
		prefix:     path.Dir(f.key),
		bucket:     f.bucket,
	})
}

// Move/Copy Operations

// CopyToFile puts the contents of File into the targetFile passed. Uses the S3 CopyObject
// method if the target file is also on S3, otherwise uses io.CopyBuffer.
func (f *File) CopyToFile(file vfs.File) (err error) {
	// Close file (f) reader regardless of an error
	defer func() {
		// close writer
		wErr := file.Close()
		// close reader
		rErr := f.Close()
		//
		if err == nil {
			if wErr != nil {
				err = wErr
			} else if rErr != nil {
				err = rErr
			}
		}
	}()
	// validate seek is at 0,0 before doing copy
	if f.cursorPos != 0 {
		return vfs.CopyToNotPossible
	}

	// if target is S3
	if tf, ok := file.(*File); ok {
		input, err := f.getCopyObjectInput(tf)
		if err != nil {
			return err
		}
		// if input is not nil, use it to natively copy object
		if input != nil {
			client, err := f.fileSystem.Client()
			if err != nil {
				return err
			}
			_, err = client.CopyObject(input)
			return err
		}
	}

	// Otherwise, use TouchCopyBuffered using io.CopyBuffer
	fileBufferSize := 0

	if opts, ok := f.Location().FileSystem().(*FileSystem).options.(Options); ok {
		fileBufferSize = opts.FileBufferSize
	}

	if err := utils.TouchCopyBuffered(file, f, fileBufferSize); err != nil {
		return err
	}
	// Close target to flush and ensure that cursor isn't at the end of the file when the caller reopens for read
	if err := file.Close(); err != nil {
		return err
	}

	return err
}

// MoveToFile puts the contents of File into the targetFile passed using File.CopyToFile.
// If the copy succeeds, the source file is deleted. Any errors from the copy or delete are
// returned.
func (f *File) MoveToFile(file vfs.File) error {
	if err := f.CopyToFile(file); err != nil {
		return err
	}

	return f.Delete()
}

// MoveToLocation works by first calling File.CopyToLocation(vfs.Location) then, if that
// succeeds, it deletes the original file, returning the new file. If the copy process fails
// the error is returned, and the Delete isn't called. If the call to Delete fails, the error
// and the file generated by the copy are both returned.
func (f *File) MoveToLocation(location vfs.Location) (vfs.File, error) {
	newFile, err := f.CopyToLocation(location)
	if err != nil {
		return nil, err
	}
	delErr := f.Delete()
	return newFile, delErr
}

// CopyToLocation creates a copy of *File, using the file's current name as the new file's
// name at the given location. If the given location is also s3, the AWS API for copying
// files will be utilized, otherwise, standard io.Copy will be done to the new file.
func (f *File) CopyToLocation(location vfs.Location) (vfs.File, error) {
	newFile, err := location.NewFile(f.Name())
	if err != nil {
		return nil, err
	}

	return newFile, f.CopyToFile(newFile)
}

// CRUD Operations

// Delete clears any local temp file, or write buffer from read/writes to the file, then makes
// a DeleteObject call to s3 for the file. If DeleteAllVersions option is provided,
// DeleteObject call is made to s3 for each version of the file. Returns any error returned by the API.
func (f *File) Delete(opts ...options.DeleteOption) error {
	if err := f.Close(); err != nil {
		return err
	}

	client, err := f.fileSystem.Client()
	if err != nil {
		return err
	}

	var deleteAllVersions bool
	for _, o := range opts {
		switch o.(type) {
		case delete.DeleteAllVersions:
			deleteAllVersions = true
		default:
		}
	}

	_, err = client.DeleteObject(&s3.DeleteObjectInput{
		Key:    &f.key,
		Bucket: &f.bucket,
	})
	if err != nil {
		return err
	}

	if deleteAllVersions {
		objectVersions, err := f.getAllObjectVersions(client)
		if err != nil {
			return err
		}

		for _, version := range objectVersions.Versions {
			if _, err = client.DeleteObject(&s3.DeleteObjectInput{
				Key:       &f.key,
				Bucket:    &f.bucket,
				VersionId: version.VersionId,
			}); err != nil {
				return err
			}
		}
	}

	return err
}

// Close cleans up underlying mechanisms for reading from and writing to the file. Closes and removes the
// local temp file, and triggers a Write to S3 of anything in the f.writeBuffer if it has been created.
func (f *File) Close() error { //nolint:gocyclo
	defer func() {
		f.reader = nil
		f.cancelFunc = nil
		f.s3Writer = nil

		// reset state
		f.cursorPos = 0
		f.seekCalled = false
		f.readCalled = false
		f.writeCalled = false
		f.readEOFSeen = false
	}()

	// cleanup reader (unless reader is also the writer tempfile)
	if f.reader != nil && !f.writeCalled {
		// close reader
		if err := f.reader.Close(); err != nil {
			return utils.WrapCloseError(err)
		}
	}

	// finalize writer
	wroteFile := false
	if f.s3Writer != nil {
		// close s3Writer
		if err := f.s3Writer.Close(); err != nil {
			return utils.WrapCloseError(err)
		}
		wroteFile = true
	} else if f.tempFileWriter != nil { // s3Writer is nil but tempFileWriter is not nil (seek after write, write after seek)
		// write tempFileWriter to s3
		if err := f.tempToS3(); err != nil {
			return utils.WrapCloseError(err)
		}
		wroteFile = true
	}

	// cleanup tempFileWriter
	if f.tempFileWriter != nil {
		if err := f.cleanupTempFile(); err != nil {
			return utils.WrapCloseError(err)
		}
	}

	// wait for file to exist
	if wroteFile {
		// read s3WriterCompleteCh if it exists
		if f.writeCalled && f.s3Writer != nil && f.s3WriterCompleteCh != nil {
			// wait for s3Writer to complete
			<-f.s3WriterCompleteCh
			// close s3WriterCompleteCh channel
			close(f.s3WriterCompleteCh)
		}
		err := waitUntilFileExists(f, 5)
		if err != nil {
			return utils.WrapCloseError(err)
		}
	}

	// close reader
	if f.reader != nil && !f.writeCalled {
		err := f.reader.Close()
		if err != nil {
			return utils.WrapCloseError(err)
		}
	}

	return nil
}

func (f *File) tempToS3() error {
	// ensure cursor is at 0
	if _, err := f.tempFileWriter.Seek(0, 0); err != nil {
		return err
	}

	// write tempFileWriter to s3
	client, err := f.fileSystem.Client()
	if err != nil {
		return err
	}

	uploader := getUploader(client, withUploadPartitionSize(f.getDownloadPartitionSize()))
	uploadInput := uploadInput(f)
	uploadInput.Body = f.tempFileWriter

	_, err = uploader.UploadWithContext(context.Background(), uploadInput)
	if err != nil {
		return err
	}

	return nil
}

// Read implements the standard for io.Reader.
func (f *File) Read(p []byte) (n int, err error) {
	// check/initialize for reader
	r, err := f.getReader()
	if err != nil {
		return 0, utils.WrapReadError(err)
	}

	read, err := r.Read(p)
	if err != nil {
		if !errors.Is(err, io.EOF) {
			return 0, utils.WrapReadError(err)
		}
		// s3 reader returns io.EOF when reading the last byte (but not past the last byte) to save on bandwidth,
		// but we want to return io.EOF only when reading past the last byte
		if f.readEOFSeen {
			return 0, io.EOF
		}
		sz, err := f.Size()
		if err != nil {
			return 0, utils.WrapReadError(err)
		}
		if f.cursorPos+int64(read) > int64(sz) {
			return read, utils.WrapReadError(err)
		}
		f.readEOFSeen = true
	}

	f.cursorPos += int64(read)
	f.readCalled = true

	return read, nil
}

func (f *File) cleanupTempFile() error {
	if f.tempFileWriter != nil {
		err := f.tempFileWriter.Close()
		if err != nil {
			return err
		}

		err = os.Remove(f.tempFileWriter.Name())
		if err != nil {
			return err
		}

		f.tempFileWriter = nil
	}

	return nil
}

// Seek implements the standard for io.Seeker.
func (f *File) Seek(offset int64, whence int) (int64, error) {
	// get length of file
	var length uint64
	if f.writeCalled {
		// if write has been called, then the length is the cursorPos
		length = uint64(f.cursorPos)
	} else {
		var err error
		length, err = f.Size()
		if err != nil {
			return 0, utils.WrapSeekError(err)
		}
	}

	// invalidate reader (if any)
	if f.reader != nil {
		err := f.reader.Close()
		if err != nil {
			return 0, utils.WrapSeekError(err)
		}

		f.reader = nil
	}

	// invalidate s3Writer
	if f.s3Writer != nil {
		// cancel s3Writer
		f.cancelFunc()
		f.cancelFunc = nil

		// close s3Writer
		err := f.s3Writer.Close()
		if err != nil {
			return 0, utils.WrapSeekError(err)
		}

		f.s3Writer = nil
	}

	// update seek position for tempFileWriter writer (if any)
	if f.tempFileWriter != nil {
		// seek tempFileWriter
		_, err := f.tempFileWriter.Seek(offset, whence)
		if err != nil {
			return 0, utils.WrapSeekError(err)
		}
	}

	// update cursorPos
	pos, err := utils.SeekTo(int64(length), f.cursorPos, offset, whence)
	if err != nil {
		return 0, utils.WrapSeekError(err)
	}
	f.cursorPos = pos

	f.seekCalled = true
	return f.cursorPos, nil
}

// Write implements the standard for io.Writer.  Note that writes are not committed to S3 until CLose() is called.
func (f *File) Write(data []byte) (int, error) {
	// check/initialize for writer
	err := f.initWriters()
	if err != nil {
		return 0, utils.WrapWriteError(err)
	}

	// write to tempfile
	written, err := f.tempFileWriter.Write(data)
	if err != nil {
		return 0, utils.WrapWriteError(err)
	}

	// write to s3
	if f.s3Writer != nil {
		// write to s3
		s3written, err := f.s3Writer.Write(data)
		if err != nil {
			return 0, utils.WrapWriteError(err)
		}

		// ensure both writes are the same
		if written != s3written {
			return 0, utils.WrapWriteError(
				fmt.Errorf("local write and s3 write are different sizes: local=%d, s3=%d", written, s3written),
			)
		}
	}

	// update cursorPos
	f.cursorPos += int64(written)
	f.writeCalled = true

	return written, nil
}

// Touch creates a zero-length file on the vfs.File if no File exists.  Update File's last modified timestamp.
// Returns error if unable to touch File.
func (f *File) Touch() error {
	// check if file exists
	exists, err := f.Exists()
	if err != nil {
		return err
	}

	// file doesn't already exist so create it
	if !exists {
		_, err = f.Write([]byte(""))
		if err != nil {
			return err
		}

		if err := f.Close(); err != nil {
			return err
		}
	} else {
		// file already exists so update its last modified date
		return utils.UpdateLastModifiedByMoving(f)
	}

	return nil
}

// URI returns the File's URI as a string.
func (f *File) URI() string {
	return utils.GetFileURI(f)
}

// String implement fmt.Stringer, returning the file's URI as the default string.
func (f *File) String() string {
	return f.URI()
}

/*
Private helper functions
*/
func (f *File) getAllObjectVersions(client s3iface.S3API) (*s3.ListObjectVersionsOutput, error) {
	prefix := utils.RemoveLeadingSlash(f.key)
	objVers, err := client.ListObjectVersions(&s3.ListObjectVersionsInput{
		Bucket: &f.bucket,
		Prefix: &prefix,
	})
	return objVers, err
}

func (f *File) getHeadObject() (*s3.HeadObjectOutput, error) {
	headObjectInput := new(s3.HeadObjectInput).SetKey(f.key).SetBucket(f.bucket)
	client, err := f.fileSystem.Client()
	if err != nil {
		return nil, err
	}

	head, err := client.HeadObject(headObjectInput)

	return head, handleExistsError(err)
}

// For copy from S3-to-S3 when credentials are the same between source and target, return *s3.CopyObjectInput or error
func (f *File) getCopyObjectInput(targetFile *File) (*s3.CopyObjectInput, error) {
	// first we must determine if we're using the same s3 credentials for source and target before doing a native copy
	isSameAccount := false
	var ACL string

	fileOptions := f.Location().FileSystem().(*FileSystem).options
	targetOptions := targetFile.Location().FileSystem().(*FileSystem).options

	if fileOptions == nil && targetOptions == nil {
		// if both opts are nil, we must be using the default credentials
		isSameAccount = true
	} else {
		opts, hasOptions := fileOptions.(Options)
		targetOpts, hasTargetOptions := targetOptions.(Options)
		if hasOptions {
			// use source ACL (even if empty), UNLESS target ACL is set
			ACL = opts.ACL
			if hasTargetOptions && targetOpts.ACL != "" {
				ACL = targetOpts.ACL
			}
			if hasTargetOptions {
				// since accesskey and session token are mutually exclusive, one will be nil
				// if both are the same, we're using the same credentials
				isSameAccount = (opts.AccessKeyID == targetOpts.AccessKeyID) && (opts.SessionToken == targetOpts.SessionToken)
			}
		}
	}

	// If both files use the same account, copy with native library. Otherwise, copy to disk
	// first before pushing out to the target file's location.
	if isSameAccount {
		// PathEscape ensures we url-encode as required by the API, including double-encoding literals
		copySourceKey := url.PathEscape(path.Join(f.bucket, f.key))

		copyInput := new(s3.CopyObjectInput).
			SetServerSideEncryption("AES256").
			SetACL(ACL).
			SetKey(targetFile.key).
			SetBucket(targetFile.bucket).
			SetCopySource(copySourceKey)

		if f.fileSystem.options != nil && f.fileSystem.options.(Options).DisableServerSideEncryption {
			copyInput.ServerSideEncryption = nil
		}

		// validate copyInput
		if err := copyInput.Validate(); err != nil {
			return nil, err
		}

		return copyInput, nil
	}

	// return nil if credentials aren't the same
	return nil, nil
}

func (f *File) copyS3ToLocalTempReader(tmpFile *os.File) error {
	client, err := f.fileSystem.Client()
	if err != nil {
		return err
	}

	// Download file
	input := new(s3.GetObjectInput).SetBucket(f.bucket).SetKey(f.key)
	opt := withDownloadPartitionSize(f.getDownloadPartitionSize())
	_, err = getDownloader(client, opt).
		DownloadWithContext(context.Background(), tmpFile, input)

	return err
}

// TODO: need to provide an implementation-agnostic container for providing config options such as SSE
func uploadInput(f *File) *s3manager.UploadInput {
	sseType := "AES256"
	input := &s3manager.UploadInput{
		Bucket:               &f.bucket,
		Key:                  &f.key,
		ServerSideEncryption: &sseType,
	}

	if f.fileSystem.options == nil {
		f.fileSystem.options = Options{}
	}

	if f.fileSystem.options.(Options).DisableServerSideEncryption {
		input.ServerSideEncryption = nil
	}

	if opts, ok := f.fileSystem.options.(Options); ok {
		if opts.ACL != "" {
			input.ACL = &opts.ACL
		}
	}

	return input
}

// WaitUntilFileExists attempts to ensure that a recently written file is available before moving on.  This is helpful for
// attempting to overcome race conditions withe S3's "eventual consistency".
// WaitUntilFileExists accepts vfs.File and an int representing the number of times to retry(once a second).
// error is returned if the file is still not available after the specified retries.
// nil is returned once the file is available.
func waitUntilFileExists(file vfs.File, retries int) error {
	// Ignore in-memory VFS files
	if _, ok := file.(*mocks.ReadWriteFile); ok {
		return nil
	}

	// Return as if file was found when retries is set to -1. Useful mainly for testing.
	if retries == -1 {
		return nil
	}
	var retryCount = 0
	for {
		if retryCount == retries {
			return fmt.Errorf("failed to find file %s after %d retries", file, retries)
		}

		// check for existing file
		found, err := file.Exists()
		if err != nil {
			return fmt.Errorf("unable to perform S3 exists on file %s: %s", file, err.Error())
		}

		if found {
			break
		}

		retryCount++
		time.Sleep(time.Second * 1)
	}

	return nil
}

func (f *File) getReader() (io.ReadCloser, error) {
	if f.reader == nil {
		if f.writeCalled && f.tempFileWriter != nil {
			// we've edited or truncated the file, so we need to read from the temp file which should already be at the
			// current cursor position
			f.reader = f.tempFileWriter
		} else {
			sz, err := f.Size()
			if err != nil {
				return nil, err
			}
			if sz == 0 {
				// can't set range on empty file, so just return an empty ReadCloser
				f.reader = io.NopCloser(strings.NewReader(""))
			} else {

				// Create the request to get the object
				input := new(s3.GetObjectInput).
					SetBucket(f.bucket).
					SetKey(f.key).
					SetRange(fmt.Sprintf("bytes=%d-", f.cursorPos))

				// Get the client
				client, err := f.fileSystem.Client()
				if err != nil {
					return nil, err
				}

				// Request the object
				result, err := client.GetObject(input)
				if err != nil {
					return nil, err
				}

				// Set the reader to the body of the object
				f.reader = result.Body
			}
		}
	}
	return f.reader, nil
}

func handleExistsError(err error) error {
	if err != nil {
		var awsErr awserr.Error
		if errors.As(err, &awsErr) {
			switch awsErr.Code() {
			case s3.ErrCodeNoSuchKey, s3.ErrCodeNoSuchBucket, "NotFound":
				return vfs.ErrNotExist
			}
		}
		return err
	}
	return nil
}

func (f *File) initWriters() error {
	if f.tempFileWriter == nil {
		// Create temp file
		tmpFile, err := os.CreateTemp("", fmt.Sprintf("vfs_s3_%s.%d", f.Name(), time.Now().UnixNano()))
		if err != nil {
			return err
		}
		f.tempFileWriter = tmpFile
		if f.cursorPos != 0 {
			// if file exists(because cursor position is non-zero), we need to copy the existing s3 file to temp
			err := f.copyS3ToLocalTempReader(tmpFile)
			if err != nil {
				return err
			}

			// seek to cursorPos
			if _, err := f.tempFileWriter.Seek(f.cursorPos, 0); err != nil {
				return err
			}
		}
	}

	// if we haven't seeked yet, we need to get the s3Writer
	if f.s3Writer == nil {
		if !f.seekCalled && !f.readCalled {
			w, err := f.getS3Writer()
			if err != nil {
				return err
			}

			// Set the reader to the body of the object
			f.s3Writer = w
		}
	}

	return nil
}

func (f *File) getS3Writer() (*io.PipeWriter, error) {
	f.s3WriterCompleteCh = make(chan struct{}, 1)
	pr, pw := io.Pipe()

	client, err := f.fileSystem.Client()
	if err != nil {
		return nil, err
	}
	uploader := getUploader(client, withUploadPartitionSize(f.getUploadPartitionSize()))
	ctx, cancel := context.WithCancel(context.Background())
	f.cancelFunc = cancel
	uploadInput := uploadInput(f)
	uploadInput.Body = pr

	go func(input *s3manager.UploadInput) {
		defer cancel()
		_, err := uploader.UploadWithContext(ctx, input)
		if err != nil {
			_ = pw.CloseWithError(err)
		}
		f.s3WriterCompleteCh <- struct{}{}
	}(uploadInput)

	return pw, nil
}

func (f *File) getUploadPartitionSize() int64 {
	partSize := defaultPartitionSize
	if f.fileSystem.options != nil {
		if opts, ok := f.fileSystem.options.(Options); ok {
			if opts.UploadPartitionSize != 0 {
				partSize = opts.UploadPartitionSize
			}
		}
	}
	return partSize
}

func (f *File) getDownloadPartitionSize() int64 {
	partSize := defaultPartitionSize
	if f.fileSystem.options != nil {
		if opts, ok := f.fileSystem.options.(Options); ok {
			if opts.DownloadPartitionSize != 0 {
				partSize = opts.DownloadPartitionSize
			}
		}
	}
	return partSize
}

func withDownloadPartitionSize(partSize int64) func(*s3manager.Downloader) {
	return func(d *s3manager.Downloader) {
		d.PartSize = partSize
	}
}

func withUploadPartitionSize(partSize int64) func(*s3manager.Uploader) {
	return func(u *s3manager.Uploader) {
		u.PartSize = partSize
	}
}

var getDownloader = func(client s3iface.S3API, opts ...func(d *s3manager.Downloader)) s3manageriface.DownloaderAPI {
	return s3manager.NewDownloaderWithClient(client, opts...)
}

var getUploader = func(client s3iface.S3API, opts ...func(d *s3manager.Uploader)) s3manageriface.UploaderAPI {
	return s3manager.NewUploaderWithClient(client, opts...)
}
